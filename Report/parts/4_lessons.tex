\chapter{Lessons Learned Perspective}

\section{Evolution and Refactoring}

In the context of a legacy Python \textit{Flask} application, the aim was to create abstractions that would minimize technical debt for potential DevOps engineers who would take over our application. After an initial major rewrite to a C\# \textit{ASP.NET} project using the Onion Architecture, which allows for easier scaling of the application, we realized that this was a time-consuming process and in real development, would have been an expensive migration. For future projects, we propose the use of a small \textit{C\#} minimum viable product (MVP) using a micro-framework like \textit{Nancy} \cite{nancyfx}, which is equivalent to \textit{Flask} for Python. This approach would help in quickly testing the application and identifying alterations in the application's functionality early on.

\section{Operation}

Operating a live cloud-based \texttt{MiniTwit} service, with several other focuses, such as other courses and jobs, has not been an easy task. However, the implementation of \texttt{Loki} and \texttt{Prometheus} with \texttt{Grafana} as a visualization tool, made the process much easier. Throughout the course, we have been seeing many errors originating from the simulator, these errors, indicated by the logging board in \texttt{Grafana}, are particularly in the form of 404 errors associated with the simulator endpoints: \textit{SimController.postMsgUsername} and \textit{SimController.FollowUser}. This situation arose due to our initial approach to the simulator part of the project. Unfortunately, we never successfully implemented the simulator tests, resulting in an incomplete configuration of our endpoints when the simulator began making API calls.

At this point, we had not yet implemented logging, which prevented us from quickly identifying these errors before the first graph of the simulator status was presented to us in class (approximately one week into the simulation). As soon as we became aware of the issue, we immediately fixed the misconfigured API endpoint. However, due to the faulty implementation at the project's start, many users had already registered wrongly. Consequently, these users continued to submit follow and tweet requests, thereby contributing to the errors observed on our \texttt{Grafana} logging board.

Despite this unfortunate start, our project's performance in the simulator status and API error graphs remained within average. However, as time progressed, we eventually emerged at the top of the error board. Concurrently, we encountered a significant number of registration errors and service breakdowns, prompting us to investigate the problem. Our findings revealed that the service would crash when registering four users simultaneously. This came to our attention as we had implemented a feature in \texttt{Grafana} that sends every developer emails when the server crashes. 

Upon closer examination, we discovered that our droplet ran out of memory during the simultaneous hashing of many usernames and passwords. Consequently, when the simulator rapidly registered new users, the service experienced repeated breakdowns. This led to an increased number of users being wrongly registered while still being able to post tweets and follow other users. 

\section{Maintenance}

To maintain the problem mentioned above, we lowered the memory consumption of our hashing allocating from \textit{131MB} to \textit{51MB} per hash. During the operation of our live system, we found another problem while looking at the monitoring in the \texttt{Grafana} dashboard. We were experiencing significant delays in response times for requests made to the API endpoint \textit{GET /Timeline}. Consequently, we initiated an investigation into the issue and discovered that the database contained over three million tweets (3,158,280). This endpoint triggered a query to the database that required sorting all the messages to display the most recent ones, i.e. every request had to sort over three million tweets, which is very time-consuming.

To maintain a good user experience within the system, we immediately made the decision to deploy an index on the message collection based on their dates\footnote{\href{https://github.com/simonskodt/itu-minitwit/pull/160/files\#diff-5eb5a300effc823f5495041811fa663ed657a763a0ff63594a760f31967e8733}{Link to \texttt{PR} for index creation}}. This optimization resulted in an improvement in response time, reducing it from an average of 2.5 seconds to only 200 milliseconds. This significant improvement was detectable through our monitoring system.

\section{DevOps Style}

Our main success for this project, as opposed to prior projects, lies in our adherence to two of the principles of The Agile Manifesto:

\begin{quote}
    \textit{“1. Our highest priority is to satisfy the customer through early and continuous delivery of valuable software. […] \newline
    3. Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.”} \cite{agilemanifesto}
\end{quote}

By releasing new features and fixes every week, we have continuously delivered valuable software to our live server, something we did not do in prior projects. This approach complements our strategy of using short-lived branches to ensure transparency in our work. Compared to previous projects, this project has had improved lead times, resulting in a faster flow. In the best cases, the time taken to complete and deploy a certain request was only a few hours.

\subsection{The Three Ways}

Reflecting on our project process in relation to the Three Ways \cite{DevOps_gates}, we can identify further improvements to enhance our DevOps implementation in future projects.

\textbf{The First Way: Flow/Systems Thinking.} By establishing a robust CI/CD chain, we were able to quickly move left-to-right work (Dev-Ops) with a low number of defects.

\textbf{The Second Way: Amplify Feedback Loops.} We incorporated a right-to-left feedback loop by monitoring and logging live system behavior on relevant metrics and critical areas of our application. By setting up a real-time email alert when our service went down, team members were quickly informed about this issue.

\textbf{The Third Way: Culture of Continual Experimentation and Learning.} We did not actively spend much time fostering a culture of learning or experimentation. However, since we have worked together on previous projects, we have developed trust in each other's work, allowing us to take risks. Our face-to-face communication and collaboration have enabled us to share knowledge, avoiding silos \cite{agilemanifesto}. We have continuously encouraged each other to make improvements to the codebase. To achieve the Third Way, we could potentially foster even more innovation and freedom to explore new ideas.

%*Database on single droplet, swarm 
%*Created droplets manually, didn't use either vagrant nor terraform